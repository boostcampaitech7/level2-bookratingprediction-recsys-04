
predict: False  # 예측할 경우 True로 설정
checkpoint: 'saved/checkpoints/Roberta.pt'   # 예측 시 불러올 모델 경로
seed: 0         # 시드 고정
device: cuda    # 가능한 값 : cpu, cuda, mps
model: RoBERTa_DeepFM       # 모델 선택



model_args:     # model에 해당하는 파라미터만 실질적으로 사용됩니다.
    RoBERTa_DeepFM:
        datatype: text      # basic, context, image, text 중 text 가능
        vector_create: False    # True: BERT를 통해 임베딩 벡터 생성 / False: 기존에 만든 벡터 사용
        embed_dim: 16           # sparse 벡터를 임베딩할 차원
        pretrained_model: 'roberta-base'   # 텍스트 임베딩에 사용할 사전학습 모델
        word_dim: 768                           # 사전학습 모델을 통해 생성된 임베딩 벡터 차원
        mlp_dims: [16, 32]      # MLP_Base의 히든 레이어 차원
        batchnorm: True    # MLP_Base에서 배치 정규화 사용 여부
        dropout: 0.2        # MLP_Base에서 드롭아웃 비율


dataset:
    data_path: data/    # 데이터셋 경로
    valid_ratio: 0.2    # Train / Vaildation split

dataloader:
    batch_size: 1024    # 배치 사이즈
    shuffle: True       # 학습 데이터 셔플 여부
    num_workers: 0      # 멀티프로세서 수. 0: 메인프로세서만 사용

optimizer:
    type: Adam      
    args:           
        lr: 0.001          # 예) 모든 옵티마이저에서 사용되는 학습률
        weight_decay: 1e-4  # 예) Adam 등 / L2 정규화 가중치
        amsgrad: False      # 예) Adam 등 / amsgrad 사용 여부

loss: RMSELoss  

lr_scheduler:
    use: False                  # True: 사용 / False: 사용하지 않음 (단, valid_ratio가 0일 경우 validation set이 없어 사용 불가)
    type: ReduceLROnPlateau    
    args:                       
        mode: 'min'             # 예) ReduceLROnPlateau / 'min' 또는 'max'
        factor: 0.1             # 예) ReduceLROnPlateau / 학습률 감소 비율
        step_size: 10           # 예) StepLR / 학습률 감소 주기 (필수)
        gamma: 0.1              # 예) StepLR 등 / 학습률 감소 비율

metrics: [ MSELoss, MAELoss, RMSELoss ]  

train:
    epochs: 20                          # 학습 에폭 수
    log_dir: saved/log                  # 로그 저장 경로
    ckpt_dir: saved/checkpoint    # 모델 저장 경로
    submit_dir: saved/submit            # 예측 저장 경로
    save_best_model: True               # True: val_loss가 최소인 모델 저장 / False: 모든 모델 저장
