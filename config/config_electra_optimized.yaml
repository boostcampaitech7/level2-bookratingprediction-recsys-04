# config_electra_optimized.yaml

memo: |-
    Optimized ELECTRA-based Text DeepFM Configuration
    - Learning rate: 1e-4 
      (5e-4가 ELECTRA fine-tuning에 더 적합한 learning rate라곤 하지만, 과적합으로 1e-4로 낮춤)
    - epoch: 30
    
    ELECTRA 모델을 사용한 Text DeepFM 실험
    - google/electra-base-discriminator 모델 사용
    - 텍스트 벡터 사용 경로: ./data/electra_vector/
    - 기존 BERT 대비 더 효율적인 학습 기대

    Feature 추가
    - user_features = ['location', 'age']
    - book_features = ['book_title', 'book_author', 'category', 'language', 'publisher', 'summary']
    

predict: False
checkpoint: 'saved/checkpoints/Text_DeepFM_ELECTRA_best.pt'
seed: 0
device: cuda # m1칩 mac 경우 'mps'
model: Text_DeepFM
wandb: False # wandb 끄고 실행.
wandb_project: 'Book_Rating_Prediction'
run_name: 'Text_DeepFM_ELECTRA_Optimized'

model_args:
    Text_DeepFM:
        datatype: text
        vector_create: True # vector 생성 옵션
        embed_dim: 64               # 증가 (16 -> 64): 더 풍부한 특성 표현
        pretrained_model: 'google/electra-base-discriminator'
        word_dim: 768
        mlp_dims: [256, 128, 64, 32]  # 더 깊은 네트워크
        batchnorm: True
        dropout: 0.3               # 약간 증가 (0.2 -> 0.3): 과적합 방지 강화

dataset:
    data_path: data/    
    valid_ratio: 0.2    

dataloader:
    batch_size: 512     # 감소 (1024 -> 512): 더 안정적인 학습
    shuffle: True       
    num_workers: 4      

optimizer:
    type: AdamW        # 변경 (Adam -> AdamW): 가중치 감쇠가 더 효과적
    args:
        lr: 1e-4       # lr 조정 (5e-4 -> 1e-4)
        weight_decay: 1e-2   # 증가 (1e-6 -> 1e-2): 더 강한 정규화
        betas: [0.9, 0.999]  # AdamW 기본값
        eps: 1e-8
        correct_bias: True    # Adam bias correction 사용

loss: RMSELoss

lr_scheduler:
    use: True
    type: CosineAnnealingWarmRestarts  # 변경: 더 효과적인 learning rate 조절
    args:
        T_0: 5                # 초기 주기 (에폭 단위)
        T_mult: 2             # 주기 증가 배율
        eta_min: 1e-6         # 최소 learning rate
        last_epoch: -1

metrics: [RMSELoss, MSELoss, MAELoss]

train:
    epochs: 30               # 증가 (20 -> 30): 충분한 학습 시간 확보
    log_dir: saved/electra_log
    ckpt_dir: saved/electra_checkpoint
    submit_dir: saved/electra_submit
    save_best_model: True
    resume: False
    resume_path: ''

    # 새로 추가: 학습 안정화를 위한 설정
    gradient_clipping: 1.0   # 그래디언트 클리핑 추가
    early_stopping_patience: 5  # 조기 종료 인내 설정
    warmup_epochs: 2        # 처음 2에폭은 learning rate를 서서히 증가